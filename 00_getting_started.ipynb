{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections\n",
    "\n",
    "We are going to solve the [NYTimes Connections words game](https://www.nytimes.com/games/connections)\n",
    "- üìö Weave documentation: https://wandb.me/weave\n",
    "- ü§ù Getting Started: https://wandb.github.io/weave/quickstart\n",
    "- üòé This code: https://github.com/wandb/connections\n",
    "- üíú Discord Channel: #capelle_experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq weave openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Project to log to\n",
    "share_results_in_public_project = False # @param {type:\"boolean\"}\n",
    "project = f\"connections\"\n",
    "if share_results_in_public_project:\n",
    "    project = \"llm-finetuning-course/connections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "print(f\"You are logging to: {project}\")\n",
    "\n",
    "weave.init(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We have created a dataset with all previous connections puzzles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/wandb/connections/main/connections_prompts.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import weave\n",
    "\n",
    "\n",
    "def load_jsonl(file_path: str) -> list: \n",
    "    return [json.loads(line) for line in open(file_path, 'r').readlines()]\n",
    "\n",
    "# ds = weave.ref('connections_prompts').get()\n",
    "ds = load_jsonl(\"connections_prompts.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[0][\"solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[0][\"words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# put your OpenAI key in the panel to the left üóùÔ∏è\n",
    "from google.colab import userdata\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# OPENAI_API_KEY = \"sk-...\"  # put your key here, the one you got from the credits üòé\n",
    "\n",
    "\n",
    "client = openai.Client(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are using the `json_object` response format to get a structured answer, we could use instructor here if we want to obtain more controlled structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def call_openai(messages, model=\"gpt-4o\", max_tokens=256, temperature=0.7):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        response_format={ \"type\": \"json_object\" }  # <- quick win to get a structured answer\n",
    "        )\n",
    "    extracted = response.choices[0].message.content\n",
    "    if extracted is None:\n",
    "        raise ValueError(\"No response from model\")\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_openai([{\"role\": \"user\", \"content\": \"What is the capital of France?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's parse the output and get a structured answer using `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@weave.op()\n",
    "def generate_solution(messages, model=\"gpt-4o\", **kwargs):\n",
    "\n",
    "    res = call_openai(messages, model=model, **kwargs)\n",
    "    try:\n",
    "        generation = json.loads(res)\n",
    "    except:\n",
    "        generation = {}\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the weave.Model class\n",
    "\n",
    "Let's organize our first model in a class, this way we can keep everything versioned and organized. [weave.Model](https://wandb.github.io/weave/guides/core-types/models) is a superclass of Pydanic BaseModel we some extra attributes, like the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShotModel(weave.Model):\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 256\n",
    "    model: str = \"gpt-4o\"\n",
    "    \n",
    "    @weave.op()\n",
    "    def predict(self, words):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt + str(list(words))}\n",
    "        ]\n",
    "        return generate_solution(messages, model=self.model, temperature=self.temperature, max_tokens=self.max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some starting prompts to use our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openAI has a system prompt that steers the conversation\n",
    "system_prompt = (\n",
    "    \"You are an expert puzzle solver. You understand literature and you are well versed on word play. \"\n",
    "    \"I want you to solve a daily word puzzle that finds commonalities between words.\\n\"\n",
    "    )\n",
    "\n",
    "# a naive prompt to solve the puzzle at once\n",
    "user_prompt = (\n",
    "    \"Here it's the puzzle:\\n\"\n",
    "    \"- There are 16 words, which form 4 groups of 4 words. Each group has some common theme that links the words.\\n\"\n",
    "    \"- You must use each of the 16 words, and use each word only once.\\n\"\n",
    "    \"- Each group of 4 words are linked together in some way. \\n\"\n",
    "    \"The connection between words can be simple.\\n\"\n",
    "    \"\"\"- An example of a simple connection would be {\"reason\":'types of fish', \"words\":[\"Bass\", \"Flounder\", \"Salmon\", \"Trout\"]}. \\n\"\"\"\n",
    "    \"\"\"- Categories can also be more complex, and require abstract or lateral thinking. An example of this type of connection would be {\"reason\": 'things that start with FIRE', \"words\": ['Ant', 'Drill', 'Island', 'Opal']}\\n\"\"\"\n",
    "    \"\"\"The results should be in JSON format as following: {\"groups\": [{\"reason\":\"reason why words are grouped\", \"words\":[\"word1\", \"word2\", \"word3\", \"word4\"]}, ...]}\"\"\"\n",
    "    \"Provide a full solution to the puzzle, it should be 4 groups of 4 words.\"\n",
    "    \"Here are the words for today‚Äôs puzzle:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneShotModel(system_prompt=system_prompt, user_prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(ds[0][\"words\"])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(words=words)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this seems fine, let's create a function to compare both results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def check_solution(solution, model_output):\n",
    "    \"Check that all group of words match the solution\"\n",
    "    solution_set = {frozenset(group[\"words\"]) for group in solution[\"groups\"]}\n",
    "    model_output_set = {frozenset(group[\"words\"]) for group in model_output[\"groups\"]}\n",
    "    \n",
    "    accuracy = len(solution_set.intersection(model_output_set))\n",
    "    \n",
    "    return {\"match\": accuracy == 4, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_solution(ds[0][\"solution\"], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and Evaluation\n",
    "\n",
    "We can automate the process of testing our model by running it on all puzzles and checking the accuracy of the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SAMPLES = 20 # the last 20 puzzles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave_eval = weave.Evaluation(dataset=ds[-NUM_TEST_SAMPLES:], scorers=[check_solution])\n",
    "await weave_eval.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn to improve this solution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
